\part{Comparison} \label{part:Comparison and implementation}

\chapter{comparisons and implementations}

This chapter will analyze the characteristics of the three algorithms on different real-world data sets. Taking into account the differences between instances (N) and dimension (M), 6 data sets with different N and M will be tested. The code will test the AUC values of the algorithms, and first analyze the individual variables. Then, analyze the variables that affect each other according to the generated heatmap. Finally, this work will take 5 representative values from each variable of each algorithm, use grid search algorithm to find the parameters that each algorithm performs best on the same data set. Analyze the performance difference of $R_{NX}$ curve.\\

\noindent As discussed above, in order to draw the conclusions in a convincing way, This work will examine the three algorithms' quality on several real-world data sets with different N and M, base on the neighborhood-based DR performance criteria discussed above. there are six public databases: Iris, Wine, Breast Cancer Wisconsin Diagnostic (BCW), Optical Recognition of Handwritten Digits test set (Digits), the Olivetti faces (Oliv) and Labeled Faces in the Wild face recognition(LFW).\\

\begin{center}
\begin{tabular}{|c|c|c|}% 通过添加 | 来表示是否需要绘制竖线
\hline  % 在表格最上方绘制横线
Datasets & Number of instances & Number of dimensions\\
\hline  %在第一行和第二行之间绘制横线
Iris & 150 & 4\\
\hline  %在第一行和第二行之间绘制横线
Wine & 178 & 13\\
\hline  %在第一行和第二行之间绘制横线
BCW & 569 & 30\\
\hline  %在第一行和第二行之间绘制横线
Oliv & 400 & 4096\\
\hline  %在第一行和第二行之间绘制横线
Digits & 1797 & 64\\
\hline  %在第一行和第二行之间绘制横线
LFW & 13233 & 5749\\
\hline % 在表格最下方绘制横线
\end{tabular}\\
\end{center}
\\

\noindent When we are analyzing the result, we should consider that the high-dimensional sphere was modeled by probability graph, the t-SNE algorithm adapts its "distance" concept to changes in regional density in the data set. As a result, it naturally expands dense clusters and shrinks sparse clusters, making the clusters roughly uniform in size. It also makes it difficult to see the relative size of clusters in the t-SNE graph, and the distance between clusters in low dimensions does not represent the distance between clusters in high dimensions.

\section{Grid search algorithm}

Grid search is a parameter adjustment method, with exhaustive strategy: among all the predetermined parameter selections, through looping and trying every possibility, the best performing parameter is the final result. The principle is only in one part. Take this thesis's work as an example. Each algorithm selects the four most important parameters, and each parameter takes 5 typical values. All possibilities are listed, which can be expressed as a 5*5*5*5 four-dimensional table. Each numerical combination is a grid, and the cycle process is like traversing and searching in each grid, so it is called grid search. As for this work, the code will go though all combination of parameters and return the best parameters considering neighborhood-based DR performance criteria.

\section{t-SNE}
There are 4 most important parameters for t-SNE, the ideas of each parameters are: 

\begin{enumerate}[1)]
\item $perplexity$: number of nearest neighbors for each point
\item $n\_iter$: maximum number of iterations for optimization
\item $early\_exaggeration$: the tightness inside clusters and the distance inbetween
\item $min\_grad\_norm$: the threshold for stopping the optimization 
\end{enumerate}\\



\subsection{$perplexity$}

\noindent $perplexity$ represent the number of neighbors each point t-SNE considers. It explains how to maintain a balance between the local and global aspects of the data. The perplexity value has a complex effect on the generated pictures, but the performance of SNE is quite reliable for changes in complexity, and the typical value of perplexity is between 5 and 50\cite{ref9}. Also, since the perplexity means the number of the neighbors, it should not be higher than the number of data points.\\

\noindent We can try different potential perplexity values in datasets with different instances. At the same time, since the algorithm using stochastic method, the parameter $random\_state$ is set to 1. This parameter determines the random number generator so that it pass an int mark for reproducible results across multiple function calls. After examining it on the first dataset Iris, we get the figures below:

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/image_comparison_tsne_perp20.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/image_comparison_tsne_perp50.png}}
\caption{LD result for t-SNE with perplecity 20 and 50}
% \label{LD result for t-SNE with n_iter 300 and 500}
\end{figure}

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/image_comparison_tsne_perp100.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/image_comparison_tsne_perp200.png}}
\caption{LD result for t-SNE with  perplecity 100 and 200}
% \label{Fig.main}
\end{figure}


\noindent In the figure, we can see that when the value is in the range of 5-50, clusters are well distinguished. Considering that the number of instances of the dataset is only 150, when the value is 100, the clusters begin to merge. When the perplexity = 200, it is difficult to distinguish different clusters. In order for the algorithm to work properly, the perplexity should actually be less than the number of points. Considering the experimental results and the strategy of the search about the optimal number of neighbor from K nearest neighbour algorithm\cite{ref12}, we can simply set perplexity to $n^{0.5}$, where n stand for number of instances. \\

\noindent At the same time, the $AUC$ value for the result with these parameters are 0.731, 0.658, 0.654, -0.004 separately. These $AUC$ values verifies the previous observations. As for the last one, it has a big difference between others and also the only minus result. With the definition of the $R_{NX}$ curve $R_{NX} (K) = ((N − 1)Q_{NX} (K) − K) /(N − 1 − K)$, this is because the K is higher than N in the formula. Numerator is relative small and denominator is a negative number, we then have a minus result.

\subsection{$min\_grad\_norm$}

This parameter is the threshold of gradient norm to determine when the optimization will be stopped. If the norm is less than the setting value, the gradient descent of Kullback-Leiber divergence will stop.

\subsection{$n\_iter$}

\noindent $n\_iter$ is the maximum number of iterations for the algorithm. The graph observed above are all generated with 1000 iteration which is the default value of $n\_iter$. In principle, the higher $n\_iter$ gives the better result. However, it may be very time consuming when it comes to big dataset. In contrast, if $n\_iter$ is too small, the algorithm does not have enough iteration to generate the convincing clusters in LD. The most important thing is to reach a stable configuration. The figure shows how the result changes with $n\_iter$ grows in Digits dataset as above:

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/t-sne/tsne_niter_300.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/t-sne/tsne_niter_500.png}}
\caption{LD result for t-SNE with n\_iter 300 and 500}
% \label{Fig.main}
\end{figure}

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/t-sne/tsne_niter_1000.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/t-sne/tsne_niter_2000.png}}
\caption{LD result for t-SNE with n\_iter 1000 and 2000}
% \label{Fig.main}
\end{figure}

\noindent From the figure, we can easily observe that the more iterations, the more well identified for clusters. These four $n\_iter$ generate $AUC$ values for 0.489, 0.526, 0.533, 0.536 separately, which verified the previous conclusion.


\subsection{$early\_exaggeration$}

$ early\_exaggeration $ is a technique in which all $p_{ij}$ are multiplied and "exaggerated" in the early stages of optimization by multipling all $p_{ij}$s a fixed number in the initial stages of the optimization. This means that almost all of the $q_{ij}$ ’s, which still add up to 1, are much too small to model their corresponding $p_{ij}$ ’s. The result of this is to force the value of $q_ {ij}$ to be more concentrated on the larger $ p_{ij} $s, especially for closer points, thereby making the earlier clusters more tightly bound together This creates a lot of relatively empty space in the map, which makes it much easier for the clusters to move around relative to one another in order to find a good global organization\cite{ref13}. By doing that, it can control the closeness of clusters in high-dimensional space and the distance between them in low-dimensional space. For larger values, the space between natural clusters will be larger in the embedded space.


\subsection{Relations between parameters}

The algorithm also generates a series of heatmaps to show the relationship of each parameter pair. The algorithm also generates a series of heatmaps to show the relationship of each parameter pair. Heatmap represents the effect of one parameter change on another parameter when other parameters take default values. According to the operation of t-SNE on 6 data sets, there are several interesting findings worth showing:

\subsubsection{$early\_exaggeration$ and $n\_iter$:}
\subsubsection{$early\_exaggeration$ and $perplexity$:}
\subsubsection{$perplexity$ and $n\_iter$:}

\subsubsection{Running time:}

As in the previous theoretical analysis,  It is easy to find that running time increase with $perplexity$ and $n\_iter$ affect the running time the most. The time increases significantly with these two variables independently with the same trend on 6 data sets. Take digits as an example, there are heatmaps for time:

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[$min\_grad\_norm$ and $n\_iter$]
{
\label{Fig.sub.1}
\includegraphics[width=4.5cm,height=4.5cm\textwidth]{images/t-sne/Digit_elapsed_time_min_grad_norm_value_n_iter_value_heatmap.png}}
% \subfigure[$perplexity$ and $early\_exaggeration$]
{
\label{Fig.sub.2}
\includegraphics[width=4.5cm,height=4.5cm\textwidth]{images/t-sne/Digit_elapsed_time_perp_value_early_exaggeration_value_heatmap.png}}
% \subfigure[$perplexity$ and $n\_iter$]
{
\label{Fig.sub.3}
\includegraphics[width=4.5cm,height=4.5cm\textwidth]{images/t-sne/Digit_elapsed_time_perp_value_n_iter_value_heatmap.png}}
\centering
\caption{Running time for perplexity and n\_iter on Digits dataset with other parameters set to default and with themselves }
% \label{Fig.main}
\end{figure}

\section{UMAP}
There are 4 most important parameters for UMAP, the ideas of each parameters are:

\begin{enumerate}[1)]
\item $n\_neighbors$: The size of local neighborhood like perplexity for t-SNE
\item $min\_dist$: The effective minimum distance between embedded points
\item $spread$: The effective scale of embedded points
\item $negative\_sample\_rate$: Number of negative samples for per positive sample
\end{enumerate}\\

\subsection{$n\_neighbors$}

The size of the local neighborhood used for manifold approximation.  Its default value is 15 instead of 30 for $perplexity$ in t-SNE. This make sense since the definition of K is $k = 2^{\sum_i p_{ij}}$ where $p_{ij} = p_{i\mid j} + p_{j\mid i} - p_{i\mid j}p_{j\mid i}$ without the log 2 function in perplexity as $perplexity = 2^{-\sum_i p_{i \mid j} \log_2 p_{i \mid j} }$. \\

\noindent The changes allows this parameter controls how UMAP balances local versus global structure in the data.It does this by constraining the size of the local neighborhood UMAP will look at when attempting to learn the manifold structure of the data. This means that low values of $n\_neighbors$ will force UMAP to concentrate on very local structure (potentially to the detriment of the big picture), while large values will push UMAP to look at larger neighborhoods of each point when estimating the manifold structure of the data, losing fine detail structure for the sake of getting the broader of the data. Also, since the $n\_neighbors$ means the number of the neighbors, it should not be higher than the number of data points.\\

\noindent We can try different potential perplexity values in datasets with different instances.With the same time as t-SNE, in order to avoid the effect of stochastic for each round, the parameter $random_stateis$ set to 1. After examining it on the dataset Digits, we get the figures below:

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_digit_n_neighbor_2.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_digit_n_neighbor_15.png}}
\caption{LD result for UMAP with n\_iter 2 and 15}
% \label{Fig.main}
\end{figure}

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_digit_n_neighbor_50.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_digit_n_neighbor_200.png}}
\caption{LD result for UMAP with n\_iter 50 and 200}
% \label{Fig.main}
\end{figure}

\noindent These values of $n\_neighbors$ lead to AUC values for 0.391, 0.449, 0.437, 0.431 seperately. As we can observe from the figure, with the number of $n\_neighbors$ increases, when constructing graphical representations of high-dimensional data, UMAP connects more and more adjacent points, which leads to a projection that more accurately reflects the global structure of the data. When it is a very low value at 2, it focus on the local structure completely. \\

\subsection{$min\_dist$}

The $min_dist$ parameter controls how tightly UMAP is allowed to pack points together. It, quite literally, provides the minimum distance apart that points are allowed to be in the low dimensional representation. This means that low values of $min_dist$ will result in clumpier embeddings. Larger values of $min_dist$ will prevent UMAP from packing point together and will focus instead on the preservation of the broad topological structure instead.\\

\noindent UMAP uses the family of curves $1 / (1+a*y^(2b))$ for modelling distance probabilities in low dimensions, not exactly Student t-distribution but very-very similar, without normalizatiom. As the formula mentioned in the theoretical part, the $min_dist$ determine the $a$ and $b$ for the curve directly:

\begin{equation*}
    {q_i_j} = (1 + a(y_i - y_j)^{2b} )^{-1} = \left\{
             \begin{array}{lr}
             1 &  y_i - y_j \leq min\_dist\\
             e^{-(y_i - y_j)-min\_dist}, & y_i - y_j \textgreater min\_dist 
             \end{array}
\right.
\end{equation*}

\noindent Here below are figures with different $min\_dist$ with the Digit dataset as above:

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_min_dist_0.0.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_min_dist_0.1.png}}
\caption{LD result for UMAP with min\_dist 0.0 and 0.1}
% \label{Fig.main}
\end{figure}

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_min_dist_0.5.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_min_dist_0.99.png}}
\caption{LD result for UMAP with min\_dist 0.5 and 0.99}
% \label{Fig.main}
\end{figure}

% 继续把min_dist搞完，之后时spread，参照网页先不考虑两者之间的关系。

\noindent From the figures above, we see that using $min\_dist$=0.0, UMAP can find smaller connected components and clusters in the data, and emphasize these features in the final embedding. As these structures in $min\_dist$ increase, the size of the clusters also increase. These structures are pushed into softer and more general functions, which provide a better overall view of the data without the loss of more detailed topology. 

\subsection{$spread$}

$spread$ control the effective scale of embedded points, which means it determines the scale at which embedded points will be spread out. whereas increasing spread keeps the shape and boundary of the clusters a bit better. spread can therefore be used to control the inter-cluster distances to some extent, where as $min\_dist$ controls the size of the clusters.

\subsection{$negative\_sample\_rate$}

This parameter determine the number of negative samples to select per positive sample in the optimization process. Increasing this value will result in greater repulsive force being applied, greater optimization cost, but slightly more accuracy. Here below are figures for different $negative\_sample\_rate$ values:

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_neg_3.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_neg_10.png}}
\caption{LD result for UMAP with min\_dist 0.0 and 0.1}
% \label{Fig.main}
\end{figure}

\begin{figure}[H]
\centering  %图片全局居中
% \subfigure[name1]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_neg_30.png}}
% \subfigure[name2]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/umap_neg_50.png}}
\caption{LD result for UMAP with min\_dist 0.5 and 0.99}
% \label{Fig.main}
\end{figure}

\noindent As we can observe, the clusters are more well identified with the increase of $negative\_sample\_rate$. As for the result, these four setting of the parameter has $AUC$ values 0.402, 0.425,  0.430, 0.449. At the same time, the running time (second) for each are 4.385, 7.749, 9.552,  25.615,  45.477 separately, which confirmed the previous argument.\\

\subsection{Relationships between algorithms and parameters}

\subsubsection{Running time}

The most obvious finding is that UMAP use much less time than t-SNE to run the code in each dataset. Here below is the comparison between UMAP and t-SNE: 

\begin{figure}[ht]

\centering
\includegraphics[width=12cm,height=7cm\textwidth]{images/image_time_umap_t-SNE.PNG}
\caption{Time consumption on different dataset for 2 algorithms
}
\label{fig:label}
\end{figure}\\

There are several reasons for this:
\begin{enumerate}[1)]

\item As defined above, the High-dimensional and low-dimensional probability is expressed as $p_{i \mid j} = e^{-\frac{d(x_i,x_j) - \rho}{\sigma_i}}$ with $p_{ij} = p_{i\mid j} + p_{j\mid i} - p_{i\mid j}p_{j\mid i}$ and 
${q_i_j} = (1 + a(y_i - y_j)^{2b} )^{-1}$, which do not apply normalization. Although they have been scaled for the segment [0,1], it turns out that there is no normalization, such as the denominator in the equation of $p_{i \mid j}$. The time for calculating high-dimensional graphs is greatly reduced, because summation or integration is a computationally expensive process. This method is like the Markov Chain Monte Carlo algorithm (MCMC). \\

\item Different with the random normal initialization used by tSNE, UMAP uses the Tulaplace operator to assign initial low-dimensional coordinates. This will reduce the UMAP change from iteration to iteration because it is no longer random initialization.\\
   
\subsubsection{The global structure preserved better using UMAP}

With a better global structure visualization result and better AUC acore, UMAP have a better balance betweeb local and global structure.\\

This firstly because of different cost function using Cross-Entropy instead of the KL-divergence. As discussed above, we have:
\begin{equation*}
    C_{EUMAP}(X,d_i_j) = \sum _{j}[-P(X) \log Q(d_i_j) + (1 - P(X)) \log (1-Q(d_i_j))]
\end{equation*}

It can turn out to be:

\begin{equation*}
\begin{aligned}
C_{EUMAP}(X,d_i_j)  &= \sum _{j}[-P(X) \log Q(d_i_j) + (1 - P(X)) \log (1-Q(d_i_j))]\\
&= e^{-X^2} \log \left[ e^{-X^2} (1 + Y^2) \right] + (1 - e^{-X^2}) \log \left[ \frac{ (1 - e^{-X^2})(1 + Y^2)}{Y^2}\right]\\
&\approx e^{-X^2} \log (1 + Y^2) + (1 - e^{-X^2}) \log (\frac{1 + Y^2}{Y^2})
\end{aligned}
\end{equation*}

This leads to the change in the preservation balance of the local-global structure. At a smaller value of $X$, we obtain the same limit as t-SNE, because the second term disappears due to the previous factor, and the logarithmic function is slower than the polynomial function:

\begin{equation*}
\limits X \to 0 :{CE(X,Y) \approx \log (1 + Y^2)} 
\end{equation*}

Therefore, in order to minimize the loss, the Y coordinate is forced to be small, that is, $Y \to 0$. This is exactly the same behavior as t-SNE. However, in the opposite limit of large $X$, the first term disappears and the former factor of the second term becomes 1, we get:

\begin{equation*}
\limits X \to \infty :{CE(X,Y) \approx \log (\frac{1 + Y^2}{Y^2})} 
\end{equation*}

If $Y$ is small, we will get a high penalty due to Y in the logarithmic denominator, so we encourage Y to be large so that the ratio under the logarithm becomes 1, and we get zero penalty. Therefore, we get $Y\to\infty$ at $X\to\infty$, so when moving from high-dimensional space to low-dimensional space, the global distance is preserved.\\

\subsubsection{Relationship between $perplexity$ and $n\_neighbor$}

t-SNE and UMAP both define the high-dimensional probability at a certain distance for observing points as:

\begin{equation*}
p_{ij} \approx e^{-\frac{(x_i - x_j)^2}{2\sigma_i^2}}
\end{equation*}

Here $\sigma$ is the parameter for how many samples can be detected by each other, which is a finite value. This means the data points over the threshold of $\sigma$ will not be considered as a candidate to analyze. Because both tSNE and UMAP are neighbor graph algorithms, the local structure of the graph is retained. However, when $\sigma\to\infty$, every point has a chance to detect every other point, which means, in principle, both t-SNE and UMAP can retain the global structure. However, $\sigma$  is not the hyperparameter of tSNE and UMAP, but as a variable of $perplexity$ and $n\_neigbor$ respectively. Here below shows how the mean $\sigma$ infected by the $perplexity$ and $n\_neighbor$ hyperparameters:

\begin{figure}[ht]

\centering
\includegraphics[width=12cm,height=7cm\textwidth]{images/image_sigmma.png}
\caption{Behavior of mean $\sigma$ as a variable of $perplexity$ and $n\_neighbor$}
\label{fig:label}
\end{figure}\\

We can easily observe that the $n\_neighbor$ hyperparameter is increased, the average sigma of UMAP will soon reach a plateau, while t-SNE is much more sensitive to $perplexity$. When it comes to big $perplexity$, the almost hyperbolic difference of the average $\sigma$ of t-SNE has a huge impact on the gradient of the tSNE cost function (KL difference). In the limit σ→∞, the high-dimensional probability in the above formula becomes 1, which leads to a decrease in the KL divergence gradient.

\subsubsection{$n\_neighbors$ and $min\_dist$}

From the previous derivation, we can basically draw the conclusion that $n\_neighbors$ change the balance between local and global structure and $min\_dist$ determine the size of the clusters. These two parameters affect the performance of the algorithm the most.\\

The figures below shows the heatmap between $n\_neighbors$ and $min\_dist$ for each dataset, it is east to find that when $min\_dist$ goes middle of the range around 0.5 and $n\_neighbors$ goes also middle of the range around 50, the performance of UMAP is the best. Except on LFW dataset, where the better result comes from two parameters are set to almost minimum of the range. This may because the size of LFW is 10 to 100 times higher in both instances and dimensions comparing with the rest of the dataset. So the algorithm need to focus more on smaller scale of the structure and keep the size of the clusters relatively small in order to have a better dimension reduction result.  

\begin{figure}[H]
\centering  %图片全局居中
\subfigure[Iris]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Iris_auc_value_n_neighbors_min_dist_heatmap.png}}
\subfigure[Wine]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Wine_auc_value_n_neighbors_min_dist_heatmap.png}}
% \caption{LD result for UMAP with n\_iter 2 and 15}
% \label{Fig.main}
\end{figure}

\begin{figure}[H]
\centering  %图片全局居中
\subfigure[BCW]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/BCW_auc_value_n_neighbors_min_dist_heatmap.png}}
\subfigure[Oliv]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Oliv_auc_value_n_neighbors_min_dist_heatmap.png}}
% \caption{LD result for UMAP with n\_iter 50 and 200}
% \label{Fig.main}
\end{figure}

\begin{figure}[H]
\centering  %图片全局居中
\subfigure[Digit]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Digit_auc_value_n_neighbors_min_dist_heatmap.png}}
\subfigure[LFW]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/LFW__auc_value_n_neighbors_min_dist_heatmap.png}}
\caption{Heatmap result with different $n\_neighbors$ and $min\_dist$ in different dataset}
% \label{Fig.main}
\end{figure}

\subsubsection{Parameters determine running time}

As for the time consumption, $n\_neighbors$ and $negative\_sample\_rate$ are two most decisive parameters with the increasement of them. Due to space limitations, here will show the time consumption tendency in the heatmap form Digits, which is the representative dataset. 

\begin{figure}[H]
\centering  %图片全局居中
\subfigure[$min\_dist$ and $negative\_sample\_rate$]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Digit_elapsed_time_min_dist_negative_sample_rate_heatmap.png}}
\subfigure[$min\_dist$ and $spread$]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Digit_elapsed_time_min_dist_spread_heatmap.png}}
% \caption{LD result for UMAP with n\_iter 2 and 15}
% \label{Fig.main}
\end{figure}

\begin{figure}[H]
\centering  %图片全局居中
\subfigure[$n\_neighbors$ and $min\_dis$]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Digit_elapsed_time_n_neighbors_min_dist_heatmap.png}}
\subfigure[$n\_neighbors$ and $negative\_sample\_rate$]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Digit_elapsed_time_n_neighbors_negative_sample_rate_heatmap.png}}
% \caption{LD result for UMAP with n\_iter 50 and 200}
% \label{Fig.main}
\end{figure}

\begin{figure}[H]
\centering  %图片全局居中
\subfigure[$n\_neighbors$ and $spread$]
{
\label{Fig.sub.1}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Digit_elapsed_time_n_neighbors_spread_heatmap.png}}
\subfigure[$spread$ and $negative\_sample\_rate$]
{
\label{Fig.sub.2}
\includegraphics[width=7cm,height=3.5cm\textwidth]{images/umap/comparison/Digit_elapsed_time_spread_negative_sample_rate_heatmap.png}}
\caption{Heatmap result with all the parameter pair in Digit dataset}
% \label{Fig.main}
\end{figure}

It is clear that $n\_neighbors$ and $negative\_sample\_rate$  play a major role in the increase of running time. When it comes to heatmap for $n\_neighbors$ and $negative\_sample\_rate$, the highest running time comes when these two parameters set to highest.

\end{enumerate}\\


