\part{Introduction} \label{part:how is the conclusion}

\chapter{Introduction}

% \section*{Dimension Reduction}

With the rapid development of information technology, many fields need to process a large amount of high-dimensional data. The speed of new data generation and storage is increasing rapidly, which leads to larger dimensions of the data. Due to the fact that high-dimensional data contains a lot of redundant information and the correlation between data is hidden in high-dimensional space, also considering that raw data are often sparse as a consequence of the curse of dimensionality, which leads to dimension disasters. As the number of features increases, the performance of the classifier will increase until the optimal number of features is reached. However, with more features added until dimension disasters happens, it will reduce the performance of the machine learning algorithm, which is also called Hughes phenomenon.\\

\noindent At present, data dimension reduction(DR) has become an important method for data mining, computer vision, machine learning, and pattern recognition to solve Hughes phenomenon and dimension disasters. The data DR method is to converts the data in the original high dimension(HD) space into a low dimension(LD) subspace, and reveals the essential distribution structure or pattern relationship of the data in the high-dimensional space. This not only reduces the time complexity of data processing and makes it easier to find data structure information, but also low-dimensional data representation easier to visualize.\\

\noindent The DR algorithms can be devided into two branches: Algorithms such as principal component analysis (PCA)\cite{ref10} and Multidimensional scaling (MDS)\cite{ref14} seek to preserve the distance structure within the data whereas algorithms like t-SNE\cite{ref13}, Isomap\cite{ref16}, LargeVis\cite{ref5}, Laplacian Eigenmaps\cite{ref19} and Uniform manifold approximation and projection(UMAP)\cite{ref17} favor the preservation of local distances over global distance.\\
\\

\noindent This thesis mainly compares three DR algorithms: Barnes-Hut(BH) t-SNE\cite{ref9}, LargeVis and UMAP. They all base on Stochastic Neighbor Embedding (SNE)\cite{ref15} algorithm but has differences on handling HD data, generating LD results as well as time consumption.\\

\noindent It searches the characteristics of the algorithms in real data sets of different instances and dimensions, and uses neighborhood-based DR performance criteria to provide method performance for evaluating the performance of the results\cite{ref4}. The work also generates heatmap for parameter pairs of each algorithm. By doing this, it discovers how each parameter changes the result and the similarities, as well as the differences between similar parameters in different algorithms and the reasons for the differences in performance between algorithms in the same data set. In the end, the optimal parameters of the three algorithms applied in each dataset with the help of the grid search algorithm, the examining code shows the visualization result with related DR performance score in 2D on each dataset  and related time consumption. The applicability of each algorithm in different situations is also summarized.\\

\noindent The content is organized as follows: $Part II$ first reviews three algorithms and the DR performance criteria used in this study. $Part III$ details the experimental comparison results and draws the concludes.


