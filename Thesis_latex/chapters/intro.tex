\part{Introduction} \label{part:how is the conclusion}

\chapter{Introduction}

% \section*{Dimension Reduction}

With the rapid development of information technology, many fields need to process a large amount of high-dimensional data. Therefore, more and more features need to be extracted from the data, which leads to larger and larger dimensions of the data. Because high-dimensional data contains a lot of redundant information and the correlation between data is hidden in high-dimensional space, considering raw data are often sparse as a consequence of the curse of dimensionality that lead to dimension disasters and Hughes phenomenon.\\

\noindent At present, data dimension reduction(DR) has become an important method for data mining, computer vision, machine learning, and pattern recognition to solve Hughes phenomenon and dimension disasters. The data DR method is based on the spectral analysis of a specific sample matrix, converts the data in the original high dimension(HD) space into a low dimension(LD) subspace, and reveals the essential distribution structure or pattern relationship of the data in the high-dimensional space through the data DR method. This not only reduces the time complexity of data processing and makes it easier to find data structure information, but also low-dimensional data representation easier to visualize.\\


\noindent The DR algorithms can be devided into two branches: Algorithms such as principal component analysis (PCA)\cite{ref10} and Multidimensional scaling (MDS)\cite{ref14} seek to preserve the distance structure within the data whereas algorithms like t-SNE\cite{ref13}, Isomap\cite{ref16}, LargeVis\cite{ref5} and Uniform manifold approximation and projection(UMAP)\cite{ref17} favor the preservation of local distances over global distance.\\

% and Laplacian Eigenmaps\cite{ref18}

\noindent This thesis mainly compares three DR algorithms: Barnes-Hut(BH) t-SNE\cite{ref9}, LargeVis and UMAP. They all base on Stochastic Neighbor Embedding (SNE)\cite{ref15} algorithm but sometimes has big different performance on  DR result and time consumption.\\

\noindent This work searches the characteristics of the algorithms in real data sets of different instances and dimensions, and uses neighborhood-based DR performance criteria to provide method performance for evaluating the performance of the results\cite{ref4}. The work also discovers the similarities and differences of similar parameters in different algorithms and the reasons for the differences in performance between algorithms in the same data set. In the end, the optimal parameters of the three algorithms applied in each dataset with the help of the grid search algorithm, the examining code shows the visualization result in 2D on each dataset and related time consumption. The applicability of each algorithm in different situations is summarized.\\

\noindent This paper is organized as follows: $Part II$ first reviews three algorithms and the DR performance criteria used in this study. $Part III$ details the experimental comparison results and draws the concludes.


